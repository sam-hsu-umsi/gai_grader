{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Prepping Submissions & Custom Criteria Descriptions\n",
    "\n",
    "You should only need to run the code blocks in Part 1 once for an assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (0.24.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (2.5.0)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (2.10)\n",
      "Requirement already satisfied: exceptiongroup in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.1.1)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2021.10.8)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (0.17.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2docx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.5.7)\n",
      "Requirement already satisfied: PyMuPDF>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdf2docx) (1.23.15)\n",
      "Requirement already satisfied: python-docx>=0.8.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdf2docx) (1.1.0)\n",
      "Requirement already satisfied: fonttools>=4.24.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdf2docx) (4.40.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdf2docx) (1.23.5)\n",
      "Requirement already satisfied: opencv-python>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdf2docx) (4.9.0.80)\n",
      "Requirement already satisfied: fire>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdf2docx) (0.5.0)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fire>=0.3.0->pdf2docx) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fire>=0.3.0->pdf2docx) (2.4.0)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from PyMuPDF>=1.19.0->pdf2docx) (1.23.9)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-docx>=0.8.10->pdf2docx) (4.9.2)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-docx>=0.8.10->pdf2docx) (4.8.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pdf2docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build custom file for manual criterion descriptions\n",
    "\n",
    "After you populate the `custom_description` column, save the file manually again with the 'UTF-8' encoding.\n",
    "For me on Windows, if I don't explicitly state that, it won't load properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Not overwriting. Change file path to save elsewhere.\n"
     ]
    }
   ],
   "source": [
    "from simpleHelper import *\n",
    "\n",
    "pathToAssgnCSV = os.path.join('data','MOVESCI_110_FA_2023_2060736_assignmentsJK.csv')\n",
    "pathToGradingsCSV = os.path.join('data','MOVESCI_110_FA_2023_2060736_gradings.csv')\n",
    "pathToRubricsCSV = os.path.join('data','MOVESCI_110_FA_2023_2060736_rubrics.csv')\n",
    "\n",
    "pathToCriterionCSV = os.path.join('data','2060736_criterion.csv')\n",
    "\n",
    "\n",
    "makeBlankCriterionFile(pathToAssgnCSV, pathToGradingsCSV, pathToRubricsCSV, pathToCriterionCSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text versions of submission files\n",
    "\n",
    "You will need to run this notebook cell to convert the submissions into a format usable by the notebook.\n",
    "\n",
    "The `data/submissions_xxxxxx` should contain the unzipped submission folders that are exported from Canvas. The folder name can be anything, you just need to specify it in the `originalSubmissionFolder` variable. You will also need the Assignment ID in variable `assignmentID`.\n",
    "\n",
    "A folder for converted submissions is automatically made called `data/Converted submissions_xxxxxx` where xxxxxx is the Assignment ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpleHelper import *\n",
    "\n",
    "originalSubmissionFolder = os.path.join('data', 'submissions_2060736')\n",
    "assignmentID = 2060736\n",
    "\n",
    "convertSubmissions(originalSubmissionFolder, assignmentID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Getting ChatGPT responses for submissions\n",
    "\n",
    "You need to provide the full file path to each of the 4 CSV files. You also need to specify your OpenAI key details, the course code, and the assignment ID.\n",
    "The `courseName` variable simply is the real name of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpleHelper import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "pathToAssgnCSV = os.path.join('data','MOVESCI_110_FA_2023_2060736_assignmentsJK.csv')\n",
    "pathToGradingsCSV = os.path.join('data','MOVESCI_110_FA_2023_2060736_gradings.csv')\n",
    "pathToRubricsCSV = os.path.join('data','MOVESCI_110_FA_2023_2060736_rubrics.csv')\n",
    "\n",
    "pathToCriterionCSV = os.path.join('data','2060736_criterion.csv')\n",
    "\n",
    "courseCode = 626158\n",
    "assignmentID = 2060736\n",
    "courseName = 'Movement Science'\n",
    "\n",
    "overWriteSave = False\n",
    "\n",
    "customDescMode = True\n",
    "critDescDF = pd.read_csv(pathToCriterionCSV).drop_duplicates()\n",
    "\n",
    "saveFolder = 'data/saves'\n",
    "errorFolder = 'data/error'\n",
    "if not os.path.exists(saveFolder):\n",
    "        os.mkdir(saveFolder)\n",
    "if not os.path.exists(errorFolder):\n",
    "        os.mkdir(errorFolder)\n",
    "    \n",
    "gradeRubricAssignmentDF = getGRAData(pathToAssgnCSV, pathToGradingsCSV, pathToRubricsCSV)\n",
    "print(gradeRubricAssignmentDF)\n",
    "print(f\"gradeRubricAssignmentDF size {gradeRubricAssignmentDF.shape[0]}\")\n",
    "print(assignmentID)\n",
    "\n",
    "gradeRubricAssignmentDF = gradeRubricAssignmentDF[gradeRubricAssignmentDF['assignment_id']==assignmentID]\n",
    "\n",
    "print(f\"gradeRubricAssignmentDF size {gradeRubricAssignmentDF.shape[0]}\")\n",
    "\n",
    "for index, row in (pbar := tqdm(gradeRubricAssignmentDF.iterrows(), total=gradeRubricAssignmentDF.shape[0])):\n",
    "    pbar.set_description(f\"Processing: {assignmentID}-{row['submitter_id']}\")\n",
    "    if checkIfSaved(row['assignment_id'], row['submitter_id'], saveFolder, errorFolder) and not overWriteSave:\n",
    "        # print(f\"Already saved: {assignmentID}-{row['submitter_id']}\")\n",
    "        continue\n",
    "    else:\n",
    "        dataDict, runSuccess = processGRARow(row, courseName, customDescMode, critDescDF)\n",
    "        saveOutputasPickle(dataDict, runSuccess, saveFolder, errorFolder)\n",
    "\n",
    "print(f\"gradeRubricAssignmentDF size {gradeRubricAssignmentDF.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install \"pandas<2.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Analyzing the results and making charts and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF = convertPicklesToDF(saveFolder)\n",
    "errorDF = convertPicklesToDF(errorFolder)\n",
    "\n",
    "excelFolder = 'data/excelJK'\n",
    "if not os.path.exists(excelFolder):\n",
    "        os.mkdir(excelFolder)\n",
    "chartFolder = 'data/chartsJK'\n",
    "if not os.path.exists(chartFolder):\n",
    "        os.mkdir(chartFolder)\n",
    "\n",
    "saveName = f'{courseName}-{assignmentID}'\n",
    "        \n",
    "mergedCriterionData = getCriterionDataDF(resultsDF, saveName, excelFolder)\n",
    "\n",
    "# getScoreSpread(resultsDF, saveName, chartFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanDiffDF, meanDiffPercentDF = saveGraderPeerGPTMeanScoreDiff(resultsDF, saveName, excelFolder)\n",
    "\n",
    "display(meanDiffPercentDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpleHelper import buildFullInfoDF\n",
    "#fullInfoDF, rubricOrderDict = buildFullInfoDF(gradeRubricAssignmentDF, resultsDF, saveName, excelFolder)\n",
    "print(\"sss\")\n",
    "critDataDF = pd.DataFrame()\n",
    "for index,row in resultsDF.iterrows():\n",
    "    criterionData = row['data_peerGPT']\n",
    "    for col in ['submitter_id', 'assignment_id', 'grader_id']:\n",
    "        criterionData[col] = row[col]\n",
    "    critDataDF = pd.concat([critDataDF, criterionData])\n",
    "allCritDF = critDataDF.drop(['mastery_points','ignore_for_scoring','title','peerGPT_criterion_id','description_grade'],\n",
    "                        axis=1, errors='ignore')\n",
    "\n",
    "print(f\"allCritDF size {allCritDF.shape[0]}\")\n",
    "\n",
    "meanInfoList = []\n",
    "for group in allCritDF.groupby(['assignment_id','criterion_id','grader_id']):\n",
    "    meanInfoList.append({'assignment_id':group[0][0], 'criterion_id':group[0][1], 'grader_id':group[0][2], \\\n",
    "                        'Grader Mean':group[1]['points_grade'].mean(), \\\n",
    "                        'Grader Std. Dev.':group[1]['points_grade'].std(), \\\n",
    "                        'peerGPT Mean':group[1]['peerGPT_criterion_score'].mean(), \\\n",
    "                        'peerGPT Std. Dev.':group[1]['peerGPT_criterion_score'].std(), \\\n",
    "                        # 'Correlation Score':group[1]['peerGPT_criterion_score'].corr(group[1]['points_grade']), \\\n",
    "                        })\n",
    "meanInfoDF = pd.DataFrame(meanInfoList)\n",
    "meanInfoDF['Mean Difference'] = meanInfoDF['peerGPT Mean'] - meanInfoDF['Grader Mean']\n",
    "print(f\"gradeRubricAssignmentDF size {gradeRubricAssignmentDF.shape[0]}\")\n",
    "assignmentDF = gradeRubricAssignmentDF[['assignment_id', 'assignment_title']].drop_duplicates()\n",
    "print(f\"assignmentDF size {assignmentDF.shape[0]}\")\n",
    "\n",
    "baseInfoDF = allCritDF[['assignment_id', 'criterion_id', 'description_rubric', 'points_rubric']].drop_duplicates()\n",
    "baseInfoDF = baseInfoDF.merge(assignmentDF, on='assignment_id')\n",
    "print(f\"baseInfoDF size {baseInfoDF.shape[0]}\")\n",
    "\n",
    "globalMeanList = [{'assignment_id':group[0][0], 'criterion_id':group[0][1], \\\n",
    "                'All Graders Mean':group[1]['points_grade'].mean(), \\\n",
    "                'All Graders Std. Dev.':group[1]['points_grade'].std(), \\\n",
    "                'Global peerGPT Mean':group[1]['peerGPT_criterion_score'].mean(), \\\n",
    "                'Global peerGPT Std. Dev.':group[1]['peerGPT_criterion_score'].std()} \\\n",
    "                    for group in allCritDF.groupby(['assignment_id', 'criterion_id'])]\n",
    "globalMeanDF = pd.DataFrame(globalMeanList)\n",
    "\n",
    "baseInfoDF = baseInfoDF.merge(globalMeanDF, on=['assignment_id', 'criterion_id'])\n",
    "\n",
    "fullInfoDF = meanInfoDF.merge(baseInfoDF, on=['assignment_id', 'criterion_id'])\n",
    "fullInfoDF['Mean Difference %'] = 100*fullInfoDF['Mean Difference'].div(fullInfoDF['points_rubric'])\n",
    "print(fullInfoDF)\n",
    "fullInfoDF['Grader Mean Diff. %'] = 100*(fullInfoDF['All Graders Mean'] - fullInfoDF['Grader Mean']).div(fullInfoDF['points_rubric'])\n",
    "print(fullInfoDF)\n",
    "print(f\"sSaving file at: {os.path.join(excelFolder, saveName+' - Grader Difference Table.xlsx')}\")\n",
    "fullInfoDF.to_excel(os.path.join(excelFolder, saveName+' - Grader Difference Table.xlsx'))\n",
    "\n",
    "rubricInfo = gradeRubricAssignmentDF[['assignment_id', 'data_rubric']].drop_duplicates('assignment_id').reset_index(drop=True)\n",
    "rubricOrderDict = {}\n",
    "for index, row in rubricInfo.iterrows():\n",
    "    rubricOrderDict[row['assignment_id']] = pd.DataFrame(row['data_rubric'])['description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getZScoreAndCI(fullInfoDF, saveName, excelFolder, confidence=0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMeanDiffPercentCharts(fullInfoDF, rubricOrderDict, courseName, chartFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
